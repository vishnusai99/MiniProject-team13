{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde202c0-0467-4bc5-9d5e-906430d0238d",
   "metadata": {},
   "source": [
    "# GNN-based Sequential Recommendation System\n",
    "\n",
    "This notebook implements a Graph Neural Network (GNN) for sequential recommendation. It covers data preprocessing, graph construction, and model training, followed by evaluation using metrics like ROC-AUC. The purpose of this notebook is to demonstrate the workflow for developing and testing a GNN-based recommendation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234fee85-486c-4c2d-9d1e-8c35e1d987d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Required Libraries\n",
    "\n",
    "Below are the essential libraries used for implementing the model, preprocessing, and evaluation functions:\n",
    "- `torch`: Deep learning framework for building neural networks.\n",
    "- `tqdm`: Used for progress bars during model training and evaluation.\n",
    "- `sklearn`: Provides utility functions for evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3737839-9b02-412b-bb76-17dda0d67b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "# Load and preprocess the dataset\n",
    "train_data = pd.read_csv(r\"/Users/vishnusai/Downloads/archive-4/train_csv_new.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be929af0-9fc3-4abc-934e-26eeb488bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.datasets.split_utils import filter_k_core\n",
    "train_data = train_data[[\"user_id\", \"product_id\", \"timestamp\"]]\n",
    "train_data.rename(columns = {'user_id':'userID','product_id':'itemID','timestamp':'time'}, inplace = True)\n",
    "train_data = filter_k_core(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "118f2b84-31ca-4397-8021-27e60e44cd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211692, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a76e5-b230-47f9-bb5b-8d513c54316f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8626868-26df-4831-81b7-5cd9024efb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the sequences is: 17.86813737778892\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_data['userID'] = label_encoder.fit_transform(train_data['userID'])\n",
    "train_data['itemID'] = label_encoder.fit_transform(train_data['itemID'])\n",
    "\n",
    "train_sequences = train_data.groupby('userID')['itemID'].apply(list).reset_index()\n",
    "\n",
    "train_sequences['sequences_length'] = train_sequences['itemID'].apply(len)\n",
    "average_length = train_sequences['sequences_length'].mean()\n",
    "print(f\"The average length of the sequences is: {average_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f33a0-9355-4a92-862c-5f3796aae91a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509428e-71eb-496b-85bc-59bb14c8da5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e167761e-347f-4d2d-b4e4-c804e3ccc35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Define the GNNRecommender class with GRU instead of LSTM\n",
    "class GNNRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim):\n",
    "        super(GNNRecommender, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.gcn1 = GCNConv(embedding_dim, embedding_dim)\n",
    "        self.gcn2 = GCNConv(embedding_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, embedding_dim, batch_first=True) \n",
    "        self.fc = nn.Linear(embedding_dim, num_items)\n",
    "\n",
    "    def forward(self, edge_index, sequences):\n",
    "        user_emb = self.user_embedding.weight\n",
    "        item_emb = self.item_embedding.weight\n",
    "        x = torch.cat([user_emb, item_emb], dim=0)\n",
    "        x = F.relu(self.gcn1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = F.relu(self.gcn2(x, edge_index))\n",
    "        seq_emb = self.item_embedding(sequences)\n",
    "        lstm_out, _ = self.lstm(seq_emb)\n",
    "        out = self.fc(lstm_out) \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b1147-e868-4e43-904b-fcee238be953",
   "metadata": {},
   "source": [
    "## Graph Construction\n",
    "\n",
    "The following function builds a graph for the input sequences. It converts sequences of interactions into a format compatible with PyTorch geometric models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dbb8b3a-0fa2-4fe7-b170-398590df8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Function to build graph from sequences\n",
    "def build_graph(sequences):\n",
    "    users = sequences['userID'].values\n",
    "    items = sequences['itemID'].apply(lambda x: x[:-1]).values\n",
    "    next_items = sequences['itemID'].apply(lambda x: x[1:]).values\n",
    "\n",
    "    # Create user-item pairs (edges)\n",
    "    user_item_pairs = [(u, i) for u, item_seq in zip(users, items) for i in item_seq]\n",
    "    \n",
    "    # Create item-item pairs (edges for sequential recommendations)\n",
    "    item_item_pairs = [(i, j) for item_seq in zip(items, next_items) for i, j in zip(item_seq[0], item_seq[1])]\n",
    "\n",
    "    # Combine user-item and item-item pairs\n",
    "    edges = user_item_pairs + item_item_pairs\n",
    "    \n",
    "\n",
    "    # Separate the edges into two lists: source and target\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    return edge_index\n",
    "\n",
    "train_graph = build_graph(train_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be53c569-29ac-4c70-95e7-2195f04884c4",
   "metadata": {},
   "source": [
    "## Padding Sequences\n",
    "\n",
    "To ensure consistent input sizes, sequences are padded to a fixed length. This is crucial for batching and feeding sequences into the GNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "445ba8d4-dec9-4be5-aeab-9fa45fdbb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def pad_sequences(seqs, max_length):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length for batching.\n",
    "    Args:\n",
    "        seqs (DataFrame): Sequences of product IDs.\n",
    "        max_length (int): Maximum length to pad sequences to.\n",
    "    Returns:\n",
    "        DataFrame: Sequences padded to the maximum length.\n",
    "    \"\"\"\n",
    "    # Convert product_id to a list of tuples if it's not already\n",
    "    seqs['itemID'] = seqs['itemID'].apply(lambda x: x if isinstance(x, tuple) else tuple(x))\n",
    "\n",
    "    # Create a new DataFrame for padded sequences\n",
    "    padded_seqs = np.zeros((len(seqs), max_length), dtype=int)\n",
    "\n",
    "    # Fill the padded_seqs with the original product_id values\n",
    "    for idx, seq in enumerate(seqs['itemID']):\n",
    "        padded_seqs[idx, :len(seq)] = seq[:max_length]\n",
    "\n",
    "    # Update the original DataFrame with padded sequences\n",
    "    seqs['itemID'] = list(map(tuple, padded_seqs))\n",
    "\n",
    "    return seqs\n",
    "\n",
    "class SequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences,max_length=10):\n",
    "        self.sequences = pad_sequences(sequences,max_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        product_ids = self.sequences.iloc[idx]['itemID']\n",
    "        if isinstance(product_ids, str):\n",
    "            product_ids = eval(product_ids)  # Convert string representation of list to list\n",
    "        return torch.tensor(product_ids, dtype=torch.long)\n",
    "\n",
    "    \n",
    "train_dataset = SequenceDataset(train_sequences)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3de38b-5e1d-4321-9925-e0a341b0fb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c45410-0adf-4be8-aa37-6c9c63f9add5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 3.502851342929984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.8693977109666141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 0.29479610103200066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 0.11140844526718248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 0.047410910509807884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "num_users = train_data['userID'].nunique()\n",
    "num_items = train_data['itemID'].nunique()\n",
    "embedding_dim = 128\n",
    "\n",
    "model = GNNRecommender(num_users, num_items, embedding_dim)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-5)\n",
    "\n",
    "from tqdm import tqdm\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc='Training', leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_graph, batch)\n",
    "        batch = batch.view(-1)  # Flatten target: (batch_size, seq_length) -> (batch_size * seq_length)\n",
    "        out = out.view(-1, num_items)  # Flatten output: (batch_size, seq_length, num_items) -> (batch_size * seq_length, num_items)\n",
    "        loss = criterion(out, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d0c3a-ea9a-4629-95ae-a9d8e8d65ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d709f-03fe-443c-a746-00547a2c96ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1f6689-5910-463d-8c27-58286d36db39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7a29b-14a7-4913-a1ff-5900ee47f853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea30370-c50e-416c-965a-1bf913433892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), 'model_state_lstm_2.pth')\n",
    "\n",
    "# Optionally, save the optimizer's state dictionary\n",
    "torch.save(optimizer.state_dict(), 'optimizer_state_lstm_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f61ea82-ba93-4502-a46c-aed2d2e77643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the sequences is: 15.091166077738515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "test_data = pd.read_parquet(r\"/Users/vishnusai/Downloads/archive-4/test.parquet\")\n",
    "test_data = test_data.sample(test_data.shape[0]//2,random_state=0)\n",
    "test_data.drop_duplicates(inplace=True)\n",
    "test_data = test_data[test_data['product_id']!=0]\n",
    "\n",
    "test_data = test_data[[\"user_id\", \"product_id\", \"timestamp\"]]\n",
    "test_data.rename(columns = {'user_id':'userID','product_id':'itemID','timestamp':'time'}, inplace = True)\n",
    "test_data = filter_k_core(test_data, 10)\n",
    "\n",
    "label_encoder3 = LabelEncoder()\n",
    "\n",
    "\n",
    "test_data['userID'] = label_encoder3.fit_transform(test_data['userID'])\n",
    "test_data['itemID'] = label_encoder3.fit_transform(test_data['itemID'])\n",
    "test_sequences = test_data.groupby('userID')['itemID'].apply(list).reset_index()\n",
    "test_sequences['sequences_length'] = test_sequences['itemID'].apply(len)\n",
    "average_length = test_sequences['sequences_length'].mean()\n",
    "print(f\"The average length of the sequences is: {average_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41528699-96da-41e4-bad4-9f298d0fb732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1216, 4245)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {}\n",
    "dataset[\"user_train\"]=dict(zip(train_sequences['userID'], train_sequences['itemID']))\n",
    "dataset[\"user_test\"]=dict(zip(test_sequences[\"userID\"],test_sequences[\"itemID\"]))\n",
    "dataset[\"usernum\"]=test_data[\"userID\"].nunique()\n",
    "dataset[\"itemnum\"]=test_data[\"itemID\"].nunique()\n",
    "dataset[\"itemnum\"],dataset[\"usernum\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "beda50f9-4261-47de-b2fb-b299751ab6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "test_graph = build_graph(test_sequences)\n",
    "test_dataset = SequenceDataset(test_sequences)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd53c55-fa50-48d5-a589-1c5bb9e3c867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0510db5d-c05e-4ee8-9ee1-bee8896a7502",
   "metadata": {},
   "source": [
    "## Model Evaluation: HR@10 and NDCG@10\n",
    "This section evaluates the model performance using the HR@10 and NDCG@10 metrics. These metrics are commonly used for ranking tasks, where the goal is to rank items based on their relevance to a given query.\n",
    "\n",
    "HR@10 (Hit Rate at 10) measures the proportion of relevant items that appear within the top 10 ranked items. It is calculated as:\n",
    "\n",
    "HR@10 = (Number of relevant items in top 10) / (Total number of relevant items)\n",
    "\n",
    "NDCG@10 (Normalized Discounted Cumulative Gain at 10) measures the relevance of the ranked items and penalizes items that are ranked too low. It is calculated as:\n",
    "\n",
    "NDCG@10 = (DCG@10) / (IDCG@10)\n",
    "\n",
    "where DCG@10 is the Discounted Cumulative Gain at 10, and IDCG@10 is the Ideal Discounted Cumulative Gain at 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d03f3cc-440c-4a5d-913d-be6ceadf7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def evaluate(model, dataset, edge_index):\n",
    "    usernum = dataset[\"usernum\"]\n",
    "    itemnum = dataset[\"itemnum\"]\n",
    "    train = dataset[\"user_train\"]\n",
    "    test = dataset[\"user_test\"]\n",
    "    \n",
    "    NDCG = 0.0\n",
    "    HT = 0.0\n",
    "    valid_user = 0.0\n",
    "    \n",
    "    if usernum > 10000:\n",
    "        users = random.sample(range(usernum), 10000)\n",
    "    else:\n",
    "        users = range(usernum)\n",
    "    er = 0\n",
    "    for u in tqdm(users, ncols=70, leave=False, unit=\"b\"):\n",
    "    \n",
    "        if len(train[u]) < 1 or len(test[u]) < 1:\n",
    "            continue\n",
    "    \n",
    "        seq = torch.zeros(10, dtype=torch.long)\n",
    "        idx = 10 - 1\n",
    "        idx -= 1\n",
    "        for i in reversed(train[u]):\n",
    "            seq[idx] = i\n",
    "            idx -= 1\n",
    "            if idx == -1:\n",
    "                break\n",
    "        rated = set(train[u])\n",
    "        rated.add(0)\n",
    "        test_item = test[u][0]\n",
    "        item_idx = [test_item]\n",
    "        for _ in range(100):\n",
    "            t = np.random.randint(1, itemnum + 1)\n",
    "            while t in rated:\n",
    "                t = np.random.randint(1, itemnum + 1)\n",
    "            item_idx.append(t)\n",
    "    \n",
    "        seq = seq.unsqueeze(0)  # Add batch dimension\n",
    "        item_idx_tensor = torch.tensor(item_idx).long()  # Convert to tensor\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model.forward(edge_index, seq)\n",
    "            predictions = predictions.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            # Extract the last sequence step's prediction (most recent interaction)\n",
    "            predictions = predictions[-1, :]  # Shape: [num_items]\n",
    "            \n",
    "            # Gather predictions for the specified item indices\n",
    "            predictions = predictions[item_idx_tensor]  # Shape: [num_candidates]\n",
    "            \n",
    "\n",
    "    \n",
    "        # Inverse to get descending sort\n",
    "        predictions = -1.0 * predictions\n",
    "        rank = predictions.argsort().argsort()\n",
    "        # print(rank,predictions)\n",
    "        # rank for the test item (the first item in item_idx)\n",
    "        test_item_rank = rank[0].item()\n",
    "\n",
    "        valid_user += 1\n",
    "    \n",
    "        if test_item_rank < 10:\n",
    "            NDCG += 1 / np.log2(test_item_rank + 2)\n",
    "            HT += 1\n",
    "    \n",
    "    return NDCG / valid_user, HT / valid_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6674b98e-be09-4010-80a5-eec6bd1d66aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1351980332556923, 0.20954063604240283)\n"
     ]
    }
   ],
   "source": [
    "from recommenders.utils.timer import Timer\n",
    "with Timer() as test_time:\n",
    "    t_test = evaluate(model,dataset,test_graph)\n",
    "print(t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe53fb-c90c-4158-8953-80ac88c67401",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to implement a GNN-based sequential recommendation system. After training, we evaluated the model's performance using the ROC-AUC score. In future iterations, hyperparameter tuning and additional metrics could be incorporated to further improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a764b-1767-414b-b338-2ee6342ed7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba9cee-45b4-425d-9f25-3ef22396bec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703f94c2-e3d0-4fa0-8ad3-ac452e1445a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed45180-68ed-4826-8051-efed8970ad2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
